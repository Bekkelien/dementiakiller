{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stuff one needs\n",
    "#! pip install SoundFile\n",
    "#! pip install pandas\n",
    "#! pip install torchaudio\n",
    "#! pip install matplotlib\n",
    "#! pip install pyaudio \n",
    "#! pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import wave\n",
    "import tarfile\n",
    "\n",
    "# As\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# From\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Pytorch\n",
    "import torchaudio\n",
    "\n",
    "# Internals|helpers\n",
    "from helpers import play_audio, extract_metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "COMBINED = False # Keep False, a tool to combine audio recordings for testing purposes only\n",
    "\n",
    "# Files\n",
    "FILE = Path('LJ025-0076.wav')\n",
    "FILE2 = Path('LJ025-0073.wav')\n",
    "METADATA = Path('metadata.csv')\n",
    "\n",
    "# Folders\n",
    "FOLDER_LJ = Path('data/LJSpeech-1.1/')\n",
    "FOLDER_WAV = Path('data/LJSpeech-1.1/wavs')\n",
    "\n",
    "# Set default stuff \n",
    "torchaudio.set_audio_backend(\"soundfile\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from LJSpeech (https://keithito.com/LJ-Speech-Dataset/)\n",
    "if FOLDER_WAV.is_dir() == False: \n",
    "    with tarfile.open(\"data/LJSpeech-1.1.tar.bz2\", \"r:bz2\") as tar:\n",
    "        tar.extractall(\"data/\")\n",
    "\n",
    "# Extracting metadata\n",
    "df_meta = extract_metadata(FOLDER_LJ / METADATA)\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all characters in the LJSpeech \n",
    "charset = []\n",
    "for text in df_meta['text']:\n",
    "    for character in text:\n",
    "        charset.append(character)\n",
    "        charset = list(dict.fromkeys(charset))\n",
    "\n",
    "charset = ''.join(sorted(charset))\n",
    "charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look into the data\n",
    "characters_max = np.max([len(x) for x in df_meta['text']])\n",
    "print(f'Max characters: {characters_max}')\n",
    "print(f'Max length for LJSpeech-1.1: {1114}') # Hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a deeper look at an example file \n",
    "waveform, sample_rate = torchaudio.load(FOLDER_WAV / FILE)\n",
    "waveform_np = waveform.numpy()\n",
    "\n",
    "plt.plot(np.arange(waveform_np.shape[1]),waveform_np[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spectrogram\n",
    "transform = torchaudio.transforms.MelSpectrogram(sample_rate, n_mels=70)\n",
    "mel_spectrogram = transform(waveform)\n",
    "\n",
    "# Find the [FILE] label matching the spectrogram \n",
    "idx = [[idx,x] for idx, x in enumerate(df_meta['filenames']) if FILE.stem in x]\n",
    "df_sample = df_meta.iloc[idx[0][0]]\n",
    "\n",
    "mel_spectrogram_log = np.log(mel_spectrogram[0])\n",
    "\n",
    "# Plot spectrogram\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "plt.title(df_sample['text'], loc='left')\n",
    "plt.imshow(mel_spectrogram_log, cmap = 'viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the audio file\n",
    "f = wave.open(str(FOLDER_WAV / FILE),\"rb\")  \n",
    "play_audio(f) \n",
    "\n",
    "if COMBINED == True:\n",
    "    wav_1 = AudioSegment.from_wav(str(FOLDER_WAV / FILE))\n",
    "    wav_2 = AudioSegment.from_wav(str(FOLDER_WAV / FILE2))\n",
    "\n",
    "    combined_sounds = wav_1 + wav_2\n",
    "    combined_sounds.export(\"data//hax.wav\", format=\"wav\")\n",
    "    f_combined = wave.open(\"data/hax.wav\",\"rb\")   \n",
    "    \n",
    "    play_audio(f_combined)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18c33b78ae6f3a75a1b5b4dc86acd528f5f4e1eb2e4105df52c34897cf835f40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
