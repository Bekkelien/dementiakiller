{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\dementiakiller\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import trange\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "\n",
    "# Training\n",
    "LR = 0.001\n",
    "MELS = 50 # FIXED due to preprocessing\n",
    "BS = 32 # DO NOT CHANGE ATM fixed to numpy folder as each bach is saved as a numpy array \n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "CHARSET = \" .,!abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "\n",
    " # TODO: Clean this up\n",
    "FOLDER_LJ = Path('data/LJSpeech-1.1/')\n",
    "FOLDER_WAV = Path('data/LJSpeech-1.1/wavs')\n",
    "\n",
    "path_numpy = FOLDER_LJ / Path('numpy')\n",
    "path_data = path_numpy / Path('data')\n",
    "path_labels = path_numpy / Path('labels')\n",
    "path_input = path_numpy / Path('input_lengths')\n",
    "path_target = path_numpy / Path('target_lengths')\n",
    "\n",
    "path_models = Path('models')\n",
    "\n",
    "path_models.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>text</th>\n",
       "      <th>text2</th>\n",
       "      <th>spectrogram</th>\n",
       "      <th>encodings</th>\n",
       "      <th>input_lengths</th>\n",
       "      <th>target_lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LJ001-0001</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "      <td>[[7.932281498312932e-09, 1.1178071872564033e-0...</td>\n",
       "      <td>[20, 22, 13, 18, 24, 13, 18, 11, 3, 1, 13, 18,...</td>\n",
       "      <td>1065</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LJ001-0002</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "      <td>[[9.356983241559647e-07, 0.0001049457350745797...</td>\n",
       "      <td>[13, 18, 1, 6, 9, 13, 18, 11, 1, 7, 19, 17, 20...</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LJ001-0003</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "      <td>[[0.00014526753511745483, 6.434485112549737e-0...</td>\n",
       "      <td>[10, 19, 22, 1, 5, 16, 24, 12, 19, 25, 11, 12,...</td>\n",
       "      <td>1066</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LJ001-0004</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "      <td>[[0.00011615546827670187, 5.416104249889031e-0...</td>\n",
       "      <td>[20, 22, 19, 8, 25, 7, 9, 8, 1, 24, 12, 9, 1, ...</td>\n",
       "      <td>567</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LJ001-0005</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "      <td>[[6.309105810942128e-05, 0.0002005760325118899...</td>\n",
       "      <td>[24, 12, 9, 1, 13, 18, 26, 9, 18, 24, 13, 19, ...</td>\n",
       "      <td>895</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    filenames                                               text  \\\n",
       "0  LJ001-0001  Printing, in the only sense with which we are ...   \n",
       "1  LJ001-0002                     in being comparatively modern.   \n",
       "2  LJ001-0003  For although the Chinese took impressions from...   \n",
       "3  LJ001-0004  produced the block books, which were the immed...   \n",
       "4  LJ001-0005  the invention of movable metal letters in the ...   \n",
       "\n",
       "                                               text2  \\\n",
       "0  Printing, in the only sense with which we are ...   \n",
       "1                     in being comparatively modern.   \n",
       "2  For although the Chinese took impressions from...   \n",
       "3  produced the block books, which were the immed...   \n",
       "4  the invention of movable metal letters in the ...   \n",
       "\n",
       "                                         spectrogram  \\\n",
       "0  [[7.932281498312932e-09, 1.1178071872564033e-0...   \n",
       "1  [[9.356983241559647e-07, 0.0001049457350745797...   \n",
       "2  [[0.00014526753511745483, 6.434485112549737e-0...   \n",
       "3  [[0.00011615546827670187, 5.416104249889031e-0...   \n",
       "4  [[6.309105810942128e-05, 0.0002005760325118899...   \n",
       "\n",
       "                                           encodings  input_lengths  \\\n",
       "0  [20, 22, 13, 18, 24, 13, 18, 11, 3, 1, 13, 18,...           1065   \n",
       "1  [13, 18, 1, 6, 9, 13, 18, 11, 1, 7, 19, 17, 20...            210   \n",
       "2  [10, 19, 22, 1, 5, 16, 24, 12, 19, 25, 11, 12,...           1066   \n",
       "3  [20, 22, 19, 8, 25, 7, 9, 8, 1, 24, 12, 9, 1, ...            567   \n",
       "4  [24, 12, 9, 1, 13, 18, 26, 9, 18, 24, 13, 19, ...            895   \n",
       "\n",
       "   target_lengths  \n",
       "0             151  \n",
       "1              30  \n",
       "2             155  \n",
       "3              89  \n",
       "4             143  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataloader \n",
    "df = pd.read_pickle(FOLDER_LJ / 'LJSpeech-1.1.pkl')\n",
    "df = df[0:500]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract train and test\n",
    "SPLIT = 0.8\n",
    "train_index = random.sample(range(len(df)), int(SPLIT*len(df)))\n",
    "test_index = list(set(list(range(len(df))))- set(train_index))\n",
    "len(test_index) + len(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train = DataLoader(df.iloc[train_index]['spectrogram'],batch_size=BS, num_workers=8)\n",
    "test = DataLoader(df.iloc[test_index]['spectrogram'],batch_size=BS, num_workers=8)\n",
    "\n",
    "#for i in train:\n",
    "#    print(i)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "# TODO: CLEAN UP THIS!\n",
    "C = 16 # BUG: Can we risk clipping for n value of C?\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "                                nn.Conv2d(1,C, kernel_size=3, stride=1, padding='same'),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm2d(C),\n",
    "                                nn.Conv2d(C,C*2, kernel_size=3, stride=1, padding='same'),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm2d(C*2),\n",
    "                                )                          \n",
    "                                \n",
    "\n",
    "        self.ln1 = nn.Sequential(\n",
    "                                nn.Linear((C*2)*50,600),\n",
    "                                nn.ReLU(),                                \n",
    "                                nn.Linear(600,300),\n",
    "                                nn.ReLU(),\n",
    "                                )\n",
    "\n",
    "        self.gru = nn.GRU(300, 300, batch_first=False)\n",
    "        self.ln2 = nn.Sequential(\n",
    "                        nn.Linear(300, 150),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(150, 75),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(75,len(CHARSET)),\n",
    "                         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Use different implementation/Input when using CONV?\n",
    "        # TODO: When using conv should we change input ? Input ATM: (Time,Batch,Mel/Freq) \n",
    "        # TODO: CLEAN UP and make sur it is correct \n",
    "        x = x.reshape(x.shape[1],1,-1,MELS)\n",
    "        print(x.shape)\n",
    "        x = self.conv(x)\n",
    "        print(x.shape)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        print(\"dis\" + str(x.shape))\n",
    "        x = x.reshape(x.shape[0],-1,(C*2)*MELS)\n",
    "        x = x.permute(1,0,2)\n",
    "        print(x.shape)\n",
    "        x = self.ln1(x)\n",
    "        print(x.shape)\n",
    "        x = nn.functional.relu(self.gru(x)[0])\n",
    "        print(x.shape)\n",
    "        x = self.ln2(x)\n",
    "        print(x.shape)\n",
    "        #  Output (Time,Batch,Mel/Freq) -> Convention when using CTC loss \n",
    "        x = torch.nn.functional.log_softmax(x, dim=2)\n",
    "        return x\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "model\n",
    "\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.ln1 = nn.Sequential(\n",
    "                                nn.Linear(50,100),\n",
    "                                nn.ReLU(),                                \n",
    "                                nn.Linear(100,250),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(250,500),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(500,500),\n",
    "                                nn.ReLU(),\n",
    "                                )\n",
    "\n",
    "        self.gru = nn.GRU(500, 500, batch_first=False)\n",
    "        self.ln2 = nn.Sequential(\n",
    "                        nn.Linear(500, 250),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(250, 75),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(75,len(CHARSET)),\n",
    "                         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = nn.functional.relu(self.gru(x)[0])\n",
    "        x = self.ln2(x)\n",
    "        x = torch.nn.functional.log_softmax(x, dim=2)\n",
    "        return x\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How are shapes with CTC loss working? \n",
    "# BUG: CNN We need to be more careful about shapes when using CTC loss due to tokens \n",
    "# INPUT [time,batch,mels]\n",
    "inn = torch.rand(1114,32,50, device=DEVICE)\n",
    "print(f\"Input shape: {inn.shape}\")\n",
    "test = model(inn)\n",
    "#\n",
    "print(f\"Output shape: {test.shape}\")\n",
    "# OUTPUT [time,batch,len(CHARSET)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion (Loss function) and Optimizer\n",
    "criterion  = nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: we need to shuffle, currently every batch and epoch are the same\n",
    "# TODO: Normalize data?\n",
    "# TODO: Char mappings how are padding handled vs space \n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "traning_loss = []\n",
    "\n",
    "t = trange(EPOCHS)\n",
    "import time\n",
    "for epoch in t:\n",
    "    for batch in range(408):\n",
    "\n",
    "        #start = time.time()\n",
    "        batch = random.randint(0, 408) # BUG: HAX: TODO: BAD\n",
    "        inputs = np.load(path_data / Path(str(batch) + '.npy'))\n",
    "        labels = np.load(path_labels / Path(str(batch) + '.npy'))\n",
    "        input_lengths = np.load(path_input / Path(str(batch) + '.npy'))\n",
    "        target_lengths = np.load(path_target / Path(str(batch) + '.npy'))\n",
    "\n",
    "        inputs = torch.tensor(inputs).reshape(-1,BS,50).float().to(DEVICE)\n",
    "        labels = torch.tensor(labels).float().to(DEVICE)\n",
    "        input_lengths = torch.tensor(input_lengths)\n",
    "        target_lengths = torch.tensor(target_lengths)\n",
    "        \n",
    "        # NOTE: 10s to load this way per epoch for LJSpeech\n",
    "        #end = time.time()\n",
    "        #print(end-start)\n",
    "        \n",
    "        #inputs = torch.nn.functional.normalize(inputs)\n",
    "\n",
    "        optimizer.zero_grad() # Reset gradient to zero\n",
    "\n",
    "        # Forward\n",
    "        output = model(inputs) \n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, labels, input_lengths,target_lengths)\n",
    "\n",
    "        # Update loss\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer\n",
    "        optimizer.step()\n",
    "        if batch % 10 == 0:\n",
    "            t.set_description(f\"Loss: {loss.item()}\")\n",
    "\n",
    "        del inputs\n",
    "        del labels\n",
    "    \n",
    "    if True:\n",
    "        torch.save(model.state_dict(), f\"models/{str(epoch)}-c.pt\")\n",
    "    \n",
    "    traning_loss.append(loss.item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(traning_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "TEST = 23 \n",
    "TEST2 = 0\n",
    "\n",
    "data = np.load(path_data / Path(str(TEST) + '.npy'))\n",
    "data = torch.tensor(data[TEST2]).float()\n",
    "data = data[:,:,None].permute(1,2,0).to(DEVICE)\n",
    "print(data.shape)\n",
    "\n",
    "# Run the model\n",
    "output = model(data)\n",
    "output = output[:, 0, :].argmax(dim=1)\n",
    "\n",
    "estimate = []\n",
    "for char in output:\n",
    "    if char != 0:\n",
    "        estimate.append(CHARSET[char-1])\n",
    "print(''.join(estimate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18c33b78ae6f3a75a1b5b4dc86acd528f5f4e1eb2e4105df52c34897cf835f40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
